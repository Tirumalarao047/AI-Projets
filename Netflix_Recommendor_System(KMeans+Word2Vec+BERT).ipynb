{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10362470,
          "sourceType": "datasetVersion",
          "datasetId": 6417893
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Netflix Recommendor System(KMeans+Word2Vec+BERT)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tirumalarao047/AI-Projets/blob/main/Netflix_Recommendor_System(KMeans%2BWord2Vec%2BBERT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "%cd /kaggle/input/netflix-movies-and-tv-shows\n",
        "df = pd.read_csv('netflix_titles.csv')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:31.023456Z",
          "iopub.execute_input": "2025-01-29T22:10:31.023814Z",
          "iopub.status.idle": "2025-01-29T22:10:31.820735Z",
          "shell.execute_reply.started": "2025-01-29T22:10:31.023791Z",
          "shell.execute_reply": "2025-01-29T22:10:31.819963Z"
        },
        "id": "_US_yyenEQ9i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:31.827451Z",
          "iopub.execute_input": "2025-01-29T22:10:31.827679Z",
          "iopub.status.idle": "2025-01-29T22:10:31.842179Z",
          "shell.execute_reply.started": "2025-01-29T22:10:31.827659Z",
          "shell.execute_reply": "2025-01-29T22:10:31.84142Z"
        },
        "id": "JjwK5NuOEQ9k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:31.843077Z",
          "iopub.execute_input": "2025-01-29T22:10:31.843359Z",
          "iopub.status.idle": "2025-01-29T22:10:31.858687Z",
          "shell.execute_reply.started": "2025-01-29T22:10:31.843309Z",
          "shell.execute_reply": "2025-01-29T22:10:31.857936Z"
        },
        "id": "ehgbXaCZEQ9l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def analyse_missing_value(df):\n",
        "    missing_value = df.isnull().sum()\n",
        "    missing_percentage = (missing_value/len(df))*100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Value': missing_value,\n",
        "        'Percentage': missing_percentage.round(2)\n",
        "    })\n",
        "    print(\"\\nMissing Value Analysis:\")\n",
        "    print(missing_df[missing_df['Missing Value']>0].sort_values('Missing Value', ascending = False))\n",
        "\n",
        "analyse_missing_value(df)\n",
        "print(\"\\n=============================================\")\n",
        "print(\"\\nContent Type Distribution:\")\n",
        "print(df['type'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:31.859422Z",
          "iopub.execute_input": "2025-01-29T22:10:31.85963Z",
          "iopub.status.idle": "2025-01-29T22:10:31.885164Z",
          "shell.execute_reply.started": "2025-01-29T22:10:31.859613Z",
          "shell.execute_reply": "2025-01-29T22:10:31.884519Z"
        },
        "id": "vuEycJZfEQ9m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Release Year Analysis\n",
        "plt.figure(figsize = (15,6))\n",
        "df['release_year'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Content Release Year Distribution')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Rating Distribution\n",
        "plt.figure(figsize = (15,6))\n",
        "df['rating'].value_counts().plot(kind = \"bar\")\n",
        "plt.title('Content Rating Distribution')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:31.885934Z",
          "iopub.execute_input": "2025-01-29T22:10:31.886294Z",
          "iopub.status.idle": "2025-01-29T22:10:32.719378Z",
          "shell.execute_reply.started": "2025-01-29T22:10:31.886263Z",
          "shell.execute_reply": "2025-01-29T22:10:32.718492Z"
        },
        "id": "hpLjOLDSEQ9n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 Countries\n",
        "def analyze_countries(df):\n",
        "    # Split multiple countries and count each occurrence\n",
        "    countries = df['country'].dropna().str.split(', ').explode()\n",
        "    top_countries = countries.value_counts().head(10)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_countries.plot(kind='bar')\n",
        "    plt.title('Top 10 Countries Producing Content')\n",
        "    plt.xlabel('Country')\n",
        "    plt.ylabel('Number of Titles')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "analyze_countries(df)\n",
        "\n",
        "# Content Categories Analysis\n",
        "def analyze_categories(df):\n",
        "    categories = df['listed_in'].dropna().str.split(', ').explode()\n",
        "    top_categories = categories.value_counts().head(15)\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    top_categories.plot(kind='bar')\n",
        "    plt.title('Top 15 Content Categories')\n",
        "    plt.xlabel('Category')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "analyze_categories(df)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:32.720316Z",
          "iopub.execute_input": "2025-01-29T22:10:32.720634Z",
          "iopub.status.idle": "2025-01-29T22:10:33.21971Z",
          "shell.execute_reply.started": "2025-01-29T22:10:32.720612Z",
          "shell.execute_reply": "2025-01-29T22:10:33.218847Z"
        },
        "id": "rcKKjNLREQ9o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_duration(df):\n",
        "    # Separate movies and TV shows\n",
        "    movies = df[df['type'] == 'Movie'].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
        "    tv_shows = df[df['type'] == 'TV Show'].copy()\n",
        "\n",
        "    # Convert movie duration to numeric (remove 'min' and convert to int)\n",
        "    movies['duration_num'] = movies['duration'].str.extract('(\\d+)').astype(float)\n",
        "\n",
        "    # Plot movie duration distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.histplot(data=movies, x='duration_num', bins=50)\n",
        "    plt.title('Movie Duration Distribution')\n",
        "    plt.xlabel('Duration (minutes)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "    # Analyze TV show seasons\n",
        "    print(\"\\nTV Show Seasons Distribution:\")\n",
        "    # Extract numbers from strings like \"2 Seasons\" using regex\n",
        "    tv_seasons = tv_shows['duration'].str.extract('(\\d+)').astype(float).squeeze()\n",
        "    season_distribution = tv_seasons.value_counts().sort_index()\n",
        "    print(season_distribution)\n",
        "\n",
        "    # Plot TV show seasons distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    season_distribution.plot(kind='bar')\n",
        "    plt.title('TV Show Seasons Distribution')\n",
        "    plt.xlabel('Number of Seasons')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "analyze_duration(df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:33.222555Z",
          "iopub.execute_input": "2025-01-29T22:10:33.222767Z",
          "iopub.status.idle": "2025-01-29T22:10:33.691296Z",
          "shell.execute_reply.started": "2025-01-29T22:10:33.222749Z",
          "shell.execute_reply": "2025-01-29T22:10:33.690467Z"
        },
        "id": "Jkv5Z1OVEQ9o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_addition_trends(df):\n",
        "    # Create a copy of the dataframe\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Clean date_added column by removing leading/trailing spaces\n",
        "    df_copy['date_added'] = df_copy['date_added'].str.strip()\n",
        "\n",
        "    # Convert date_added to datetime with mixed format\n",
        "    df_copy['date_added'] = pd.to_datetime(df_copy['date_added'], format='mixed')\n",
        "\n",
        "    # Group by month and year\n",
        "    monthly_additions = df_copy.groupby(df_copy['date_added'].dt.to_period('M')).size()\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    monthly_additions.plot(kind='line', marker='o', markersize=4)\n",
        "    plt.title('Content Addition Trends Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Titles Added')\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print some statistics about content additions\n",
        "    print(\"\\nContent Addition Statistics:\")\n",
        "    print(f\"Peak month: {monthly_additions.idxmax()} with {monthly_additions.max()} titles\")\n",
        "    print(f\"Average monthly additions: {monthly_additions.mean():.1f} titles\")\n",
        "\n",
        "def analyze_directors(df):\n",
        "    # Get top directors by number of titles\n",
        "    top_directors = df['director'].dropna().str.split(', ').explode().value_counts().head(10)\n",
        "\n",
        "    # Create a more visually appealing plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(top_directors)))\n",
        "    bars = top_directors.plot(kind='bar', color=colors)\n",
        "    plt.title('Top 10 Directors by Number of Titles', pad=20)\n",
        "    plt.xlabel('Director')\n",
        "    plt.ylabel('Number of Titles')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for i, v in enumerate(top_directors):\n",
        "        plt.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print additional director statistics\n",
        "    print(\"\\nDirector Statistics:\")\n",
        "    print(f\"Total number of unique directors: {df['director'].dropna().str.split(', ').explode().nunique()}\")\n",
        "    print(\"\\nTop 10 Directors and their title counts:\")\n",
        "    for director, count in top_directors.items():\n",
        "        print(f\"{director}: {count} titles\")\n",
        "\n",
        "# Function to show more detailed summary statistics\n",
        "def show_summary_statistics(df):\n",
        "    print(\"\\nSummary Statistics for Numeric Columns:\")\n",
        "\n",
        "    # Basic statistics for release_year\n",
        "    print(\"\\nRelease Year Statistics:\")\n",
        "    print(df['release_year'].describe())\n",
        "\n",
        "    # Content type distribution\n",
        "    print(\"\\nContent Type Distribution:\")\n",
        "    print(df['type'].value_counts())\n",
        "\n",
        "    # Rating distribution\n",
        "    print(\"\\nRating Distribution:\")\n",
        "    print(df['rating'].value_counts())\n",
        "\n",
        "    # Calculate percentage of missing values\n",
        "    missing_data = (df.isnull().sum() / len(df)) * 100\n",
        "    print(\"\\nPercentage of Missing Values:\")\n",
        "    print(missing_data[missing_data > 0].round(2))\n",
        "\n",
        "# Run the analyses\n",
        "analyze_addition_trends(df)\n",
        "analyze_directors(df)\n",
        "show_summary_statistics(df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:33.692778Z",
          "iopub.execute_input": "2025-01-29T22:10:33.693015Z",
          "iopub.status.idle": "2025-01-29T22:10:34.411396Z",
          "shell.execute_reply.started": "2025-01-29T22:10:33.692995Z",
          "shell.execute_reply": "2025-01-29T22:10:34.41055Z"
        },
        "id": "IH3w5t5TEQ9p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nUnique Values in Each Column:\")\n",
        "for column in df.columns:\n",
        "    print(f\"\\n{column}: {df[column].nunique()} unique values\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:34.412252Z",
          "iopub.execute_input": "2025-01-29T22:10:34.412538Z",
          "iopub.status.idle": "2025-01-29T22:10:34.433849Z",
          "shell.execute_reply.started": "2025-01-29T22:10:34.412506Z",
          "shell.execute_reply": "2025-01-29T22:10:34.433226Z"
        },
        "id": "1kuIoBqrEQ9q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from scipy.sparse import hstack"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:34.434541Z",
          "iopub.execute_input": "2025-01-29T22:10:34.434742Z",
          "iopub.status.idle": "2025-01-29T22:10:57.619806Z",
          "shell.execute_reply.started": "2025-01-29T22:10:34.434726Z",
          "shell.execute_reply": "2025-01-29T22:10:57.6188Z"
        },
        "id": "pig74nk3EQ9q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:57.620756Z",
          "iopub.execute_input": "2025-01-29T22:10:57.621445Z",
          "iopub.status.idle": "2025-01-29T22:10:57.686109Z",
          "shell.execute_reply.started": "2025-01-29T22:10:57.621418Z",
          "shell.execute_reply": "2025-01-29T22:10:57.685272Z"
        },
        "id": "wvULzWJeEQ9q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class NetflixRecommender:\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.df['description'] = self.df['description'].fillna('')\n",
        "        self.df['director'] = self.df['director'].fillna('')\n",
        "        self.df['cast'] = self.df['cast'].fillna('')\n",
        "        self.df['listed_in'] = self.df['listed_in'].fillna('')\n",
        "\n",
        "        # Create a combined features column\n",
        "        self.df['combined_features'] = self.create_combined_features()\n",
        "\n",
        "        # Create TF-IDF matrix\n",
        "        self.tfidf = TfidfVectorizer(stop_words='english')\n",
        "        self.tfidf_matrix = self.tfidf.fit_transform(self.df['combined_features'])\n",
        "\n",
        "        # Calculate similarity matrix\n",
        "        self.similarity_matrix = cosine_similarity(self.tfidf_matrix)\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text data\"\"\"\n",
        "        # Convert to lowercase and remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = text.split()\n",
        "        words = [w for w in words if w not in stop_words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def create_combined_features(self):\n",
        "        \"\"\"Combine different features into a single string\"\"\"\n",
        "        return (self.df['description'].apply(self.clean_text) + ' ' +\n",
        "                self.df['director'].apply(self.clean_text) + ' ' +\n",
        "                self.df['cast'].apply(self.clean_text) + ' ' +\n",
        "                self.df['listed_in'].apply(self.clean_text))\n",
        "\n",
        "    def get_recommendations(self, title, n_recommendations=5):\n",
        "        \"\"\"Get n_recommendations similar titles based on the input title\"\"\"\n",
        "        try:\n",
        "            # Find the index of the movie/show\n",
        "            idx = self.df[self.df['title'] == title].index[0]\n",
        "\n",
        "            # Get similarity scores for all titles\n",
        "            similarity_scores = list(enumerate(self.similarity_matrix[idx]))\n",
        "\n",
        "            # Sort based on similarity scores\n",
        "            similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Get top n similar titles (excluding the input title)\n",
        "            similar_titles = similarity_scores[1:n_recommendations+1]\n",
        "\n",
        "            recommendations = []\n",
        "            for i, score in similar_titles:\n",
        "                recommendations.append({\n",
        "                    'title': self.df.iloc[i]['title'],\n",
        "                    'type': self.df.iloc[i]['type'],\n",
        "                    'description': self.df.iloc[i]['description'],\n",
        "                    'similarity_score': score,\n",
        "                    'genre': self.df.iloc[i]['listed_in']\n",
        "                })\n",
        "\n",
        "            return recommendations\n",
        "\n",
        "        except IndexError:\n",
        "            return f\"Title '{title}' not found in the dataset.\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:57.687013Z",
          "iopub.execute_input": "2025-01-29T22:10:57.687284Z",
          "iopub.status.idle": "2025-01-29T22:10:57.695992Z",
          "shell.execute_reply.started": "2025-01-29T22:10:57.687254Z",
          "shell.execute_reply": "2025-01-29T22:10:57.695285Z"
        },
        "id": "o1nbfLGiEQ9r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('netflix_titles.csv')\n",
        "\n",
        "# Initialize recommender\n",
        "recommender = NetflixRecommender(df)\n",
        "\n",
        "# Example: Get recommendations for a specific title\n",
        "test_title = \"Stranger Things\"\n",
        "recommendations = recommender.get_recommendations(test_title)\n",
        "\n",
        "# Print recommendations\n",
        "print(f\"\\nRecommendations based on '{test_title}':\")\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"\\n{i}. {rec['title']}\")\n",
        "    print(f\"Type: {rec['type']}\")\n",
        "    print(f\"Genre: {rec['genre']}\")\n",
        "    print(f\"Similarity Score: {rec['similarity_score']:.2f}\")\n",
        "    print(f\"Description: {rec['description'][:100]}...\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:10:57.696807Z",
          "iopub.execute_input": "2025-01-29T22:10:57.697422Z",
          "iopub.status.idle": "2025-01-29T22:11:03.993613Z",
          "shell.execute_reply.started": "2025-01-29T22:10:57.697384Z",
          "shell.execute_reply": "2025-01-29T22:11:03.99269Z"
        },
        "id": "7cQR__tGEQ9r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedNetflixRecommender:\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        # Fill NaN values\n",
        "        self.df['description'] = self.df['description'].fillna('')\n",
        "        self.df['director'] = self.df['director'].fillna('')\n",
        "        self.df['cast'] = self.df['cast'].fillna('')\n",
        "        self.df['listed_in'] = self.df['listed_in'].fillna('')\n",
        "\n",
        "        # Download required NLTK data\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        # Initialize models\n",
        "        self.bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "        self.init_models()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        tokens = word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [t for t in tokens if t not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def init_models(self):\n",
        "        \"\"\"Initialize all component models\"\"\"\n",
        "        # Process descriptions with BERT\n",
        "        print(\"Processing descriptions with BERT...\")\n",
        "        self.description_embeddings = self.bert_model.encode(self.df['description'].tolist())\n",
        "\n",
        "        # Create TF-IDF vectors for different features\n",
        "        print(\"Creating TF-IDF vectors...\")\n",
        "        self.tfidf = TfidfVectorizer(stop_words='english')\n",
        "        self.genre_matrix = self.tfidf.fit_transform(self.df['listed_in'])\n",
        "\n",
        "        # Train Word2Vec on descriptions\n",
        "        print(\"Training Word2Vec model...\")\n",
        "        descriptions_tokens = [word_tokenize(self.clean_text(desc))\n",
        "                             for desc in self.df['description']]\n",
        "        self.word2vec_model = Word2Vec(sentences=descriptions_tokens,\n",
        "                                     vector_size=1000,\n",
        "                                     window=10,\n",
        "                                     min_count=1)\n",
        "\n",
        "        # Create description embeddings using Word2Vec\n",
        "        self.desc_w2v_embeddings = np.array([\n",
        "            np.mean([self.word2vec_model.wv[word]\n",
        "                    for word in word_tokenize(self.clean_text(desc))\n",
        "                    if word in self.word2vec_model.wv]\n",
        "                   or [np.zeros(1000)], axis=0)\n",
        "            for desc in self.df['description']\n",
        "        ])\n",
        "\n",
        "        # Cluster similar content\n",
        "        print(\"Clustering content...\")\n",
        "        self.n_clusters = 10\n",
        "        combined_features = np.concatenate([\n",
        "            self.description_embeddings,\n",
        "            self.desc_w2v_embeddings\n",
        "        ], axis=1)\n",
        "\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
        "        self.clusters = self.kmeans.fit_predict(combined_features)\n",
        "        self.df['cluster'] = self.clusters\n",
        "\n",
        "    def get_description_similarity(self, idx):\n",
        "        \"\"\"Get similarity scores based on BERT description embeddings\"\"\"\n",
        "        return cosine_similarity(\n",
        "            self.description_embeddings[idx].reshape(1, -1),\n",
        "            self.description_embeddings\n",
        "        ).flatten()\n",
        "\n",
        "    def get_genre_similarity(self, idx):\n",
        "        \"\"\"Get similarity scores based on genre TF-IDF\"\"\"\n",
        "        return cosine_similarity(\n",
        "            self.genre_matrix[idx],\n",
        "            self.genre_matrix\n",
        "        ).flatten()\n",
        "\n",
        "    def get_w2v_similarity(self, idx):\n",
        "        \"\"\"Get similarity scores based on Word2Vec embeddings\"\"\"\n",
        "        return cosine_similarity(\n",
        "            self.desc_w2v_embeddings[idx].reshape(1, -1),\n",
        "            self.desc_w2v_embeddings\n",
        "        ).flatten()\n",
        "\n",
        "    def get_recommendations(self, title, n_recommendations=5, weights=None):\n",
        "        \"\"\"\n",
        "        Get recommendations using weighted combination of different similarity metrics\n",
        "\n",
        "        Parameters:\n",
        "        - title: str, title to base recommendations on\n",
        "        - n_recommendations: int, number of recommendations to return\n",
        "        - weights: dict, weights for different similarity components\n",
        "                  (default: {'description': 0.4, 'genre': 0.3, 'w2v': 0.3})\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Default weights if none provided\n",
        "            if weights is None:\n",
        "                weights = {\n",
        "                    'description': 0.4,\n",
        "                    'genre': 0.3,\n",
        "                    'w2v': 0.3\n",
        "                }\n",
        "\n",
        "            # Find index of the title\n",
        "            idx = self.df[self.df['title'] == title].index[0]\n",
        "            cluster = self.clusters[idx]\n",
        "\n",
        "            # Get similarity scores from different components\n",
        "            desc_similarity = self.get_description_similarity(idx)\n",
        "            genre_similarity = self.get_genre_similarity(idx)\n",
        "            w2v_similarity = self.get_w2v_similarity(idx)\n",
        "\n",
        "            # Combine similarity scores with weights\n",
        "            combined_similarity = (\n",
        "                weights['description'] * desc_similarity +\n",
        "                weights['genre'] * genre_similarity +\n",
        "                weights['w2v'] * w2v_similarity\n",
        "            )\n",
        "\n",
        "            # Boost scores for items in the same cluster\n",
        "            cluster_boost = 0.2\n",
        "            combined_similarity[self.clusters == cluster] += cluster_boost\n",
        "\n",
        "            # Get top similar indices (excluding the input title)\n",
        "            similar_indices = combined_similarity.argsort()[::-1][1:n_recommendations+1]\n",
        "\n",
        "            recommendations = []\n",
        "            for i in similar_indices:\n",
        "                recommendations.append({\n",
        "                    'title': self.df.iloc[i]['title'],\n",
        "                    'type': self.df.iloc[i]['type'],\n",
        "                    'description': self.df.iloc[i]['description'],\n",
        "                    'genre': self.df.iloc[i]['listed_in'],\n",
        "                    'similarity_score': combined_similarity[i],\n",
        "                    'cluster': self.clusters[i]\n",
        "                })\n",
        "\n",
        "            return recommendations\n",
        "\n",
        "        except IndexError:\n",
        "            return f\"Title '{title}' not found in the dataset.\"\n",
        "\n",
        "    def get_cluster_recommendations(self, cluster_id, n_recommendations=5):\n",
        "        \"\"\"Get top recommendations from a specific cluster\"\"\"\n",
        "        cluster_items = self.df[self.df['cluster'] == cluster_id]\n",
        "        return cluster_items.sample(min(n_recommendations, len(cluster_items)))\n",
        "\n",
        "    def analyze_user_preferences(self, watched_titles):\n",
        "        \"\"\"Analyze user preferences based on watch history\"\"\"\n",
        "        watched_indices = [self.df[self.df['title'] == title].index[0]\n",
        "                          for title in watched_titles if title in self.df['title'].values]\n",
        "\n",
        "        if not watched_indices:\n",
        "            return \"No valid titles found in watch history.\"\n",
        "\n",
        "        # Get average embeddings for watched content\n",
        "        avg_desc_embedding = np.mean(self.description_embeddings[watched_indices], axis=0)\n",
        "        avg_w2v_embedding = np.mean(self.desc_w2v_embeddings[watched_indices], axis=0)\n",
        "\n",
        "        # Find most common cluster\n",
        "        preferred_cluster = self.df.iloc[watched_indices]['cluster'].mode()[0]\n",
        "\n",
        "        # Get genre preferences\n",
        "        genre_counts = (self.df.iloc[watched_indices]['listed_in']\n",
        "                       .str.split(', ')\n",
        "                       .explode()\n",
        "                       .value_counts())\n",
        "\n",
        "        return {\n",
        "            'preferred_cluster': preferred_cluster,\n",
        "            'top_genres': genre_counts.head().to_dict(),\n",
        "            'avg_embeddings': {\n",
        "                'description': avg_desc_embedding,\n",
        "                'w2v': avg_w2v_embedding\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:11:03.994637Z",
          "iopub.execute_input": "2025-01-29T22:11:03.994969Z",
          "iopub.status.idle": "2025-01-29T22:11:04.010121Z",
          "shell.execute_reply.started": "2025-01-29T22:11:03.994937Z",
          "shell.execute_reply": "2025-01-29T22:11:04.009196Z"
        },
        "id": "J3OQqoOREQ9s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Read the dataset\n",
        "#     df = pd.read_csv('netflix_titles.csv')\n",
        "\n",
        "#     # Initialize recommender\n",
        "#     print(\"Initializing recommender system...\")\n",
        "#     recommender = AdvancedNetflixRecommender(df)\n",
        "\n",
        "#     # Example: Get recommendations for a specific title\n",
        "#     test_title = \"Naruto\"\n",
        "#     print(f\"\\nGetting recommendations for '{test_title}'...\")\n",
        "#     recommendations = recommender.get_recommendations(test_title)\n",
        "\n",
        "#     # Print recommendations\n",
        "#     for i, rec in enumerate(recommendations, 1):\n",
        "#         print(f\"\\n{i}. {rec['title']}\")\n",
        "#         print(f\"Type: {rec['type']}\")\n",
        "#         print(f\"Genre: {rec['genre']}\")\n",
        "#         print(f\"Similarity Score: {rec['similarity_score']:.2f}\")\n",
        "#         print(f\"Cluster: {rec['cluster']}\")\n",
        "\n",
        "#     # Example: Analyze user preferences\n",
        "#     watched_titles = [\"Naruto\", \"Black Clover\", \"Bleach\"]\n",
        "#     print(\"\\nAnalyzing user preferences...\")\n",
        "#     preferences = recommender.analyze_user_preferences(watched_titles)\n",
        "#     print(\"\\nUser Preferences Analysis:\")\n",
        "#     print(f\"Preferred Cluster: {preferences['preferred_cluster']}\")\n",
        "#     print(\"\\nTop Genres:\")\n",
        "#     for genre, count in preferences['top_genres'].items():\n",
        "#         print(f\"- {genre}: {count}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:11:04.010878Z",
          "iopub.execute_input": "2025-01-29T22:11:04.011088Z",
          "iopub.status.idle": "2025-01-29T22:11:04.027238Z",
          "shell.execute_reply.started": "2025-01-29T22:11:04.011071Z",
          "shell.execute_reply": "2025-01-29T22:11:04.026589Z"
        },
        "id": "lObt8YkBEQ9t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('netflix_titles.csv')\n",
        "\n",
        "# Initialize recommender\n",
        "print(\"Initializing recommender system...\")\n",
        "recommender = AdvancedNetflixRecommender(df)\n",
        "\n",
        "# Example: Get recommendations for a specific title\n",
        "test_title = \"Naruto\"\n",
        "print(f\"\\nGetting recommendations for '{test_title}'...\")\n",
        "recommendations = recommender.get_recommendations(test_title)\n",
        "\n",
        "# Print recommendations\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"\\n{i}. {rec['title']}\")\n",
        "    print(f\"Type: {rec['type']}\")\n",
        "    print(f\"Genre: {rec['genre']}\")\n",
        "    print(f\"Similarity Score: {rec['similarity_score']:.2f}\")\n",
        "    print(f\"Cluster: {rec['cluster']}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:11:04.028032Z",
          "iopub.execute_input": "2025-01-29T22:11:04.028324Z",
          "iopub.status.idle": "2025-01-29T22:11:30.873523Z",
          "shell.execute_reply.started": "2025-01-29T22:11:04.028294Z",
          "shell.execute_reply": "2025-01-29T22:11:30.872613Z"
        },
        "id": "JTzBWrPUEQ9t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_title = \"Vincenzo\"\n",
        "\n",
        "recommendations = recommender.get_recommendations(test_title)\n",
        "\n",
        "    # Print recommendations\n",
        "for i, rec in enumerate(recommendations, 1):\n",
        "    print(f\"\\n{i}. {rec['title']}\")\n",
        "    print(f\"Type: {rec['type']}\")\n",
        "    print(f\"Genre: {rec['genre']}\")\n",
        "    print(f\"Similarity Score: {rec['similarity_score']:.2f}\")\n",
        "    print(f\"Cluster: {rec['cluster']}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-29T22:11:30.874438Z",
          "iopub.execute_input": "2025-01-29T22:11:30.874786Z",
          "iopub.status.idle": "2025-01-29T22:11:30.935213Z",
          "shell.execute_reply.started": "2025-01-29T22:11:30.874753Z",
          "shell.execute_reply": "2025-01-29T22:11:30.934249Z"
        },
        "id": "dvd0YGTBEQ9t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "-X5X1DazEQ9u"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}